{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dwGRQchYQin6",
        "XYQIXxf_Quro",
        "vDOahv36RZj9",
        "_7hW564KR5Jl",
        "9TVo7fqmSJw3",
        "AGdbah8SaCVp",
        "QVsAj3k0sGAQ",
        "5KBq_E-MyJlj"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX41gGpz-bwH"
      },
      "source": [
        "crypto_key = \"YOUR_CRYPTO_API_KEY\"\n",
        "stocks_key = \"YOUR_STOCKS_API_KEY\"\n",
        "agg_bar_size = \"1\"\n",
        "agg_bar_unit = \"minute\"\n",
        "\n",
        "is_crypto = False\n",
        "tickers = [\"SPY\"]\n",
        "start_date = \"2015-01-01\"\n",
        "end_date = \"2023-03-18\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwGRQchYQin6"
      },
      "source": [
        "#Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDZYBDM4AfMD"
      },
      "source": [
        "%%capture\n",
        "!pip install aiohttp aiodns\n",
        "!pip install pandas_market_calendars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "oYzltIMxQhz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYQIXxf_Quro"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC2OTSg55-un"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_market_calendars as mcal\n",
        "from datetime import datetime, date, time, timedelta\n",
        "import requests\n",
        "import os\n",
        "import glob\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "import aiohttp  # \n",
        "import datetime\n",
        "import warnings\n",
        "from dateutil.relativedelta import *\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pytz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDOahv36RZj9"
      },
      "source": [
        "# Download data from Polygon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U32dUCS9KWt"
      },
      "source": [
        "def daterange(date1, date2):\n",
        "    for n in range(int((date2 - date1).days) + 1):\n",
        "        yield date1 + timedelta(n)\n",
        "\n",
        "data_dict = {}\n",
        "\n",
        "async def get(\n",
        "    session: aiohttp.ClientSession,\n",
        "    date: str,\n",
        "    **kwargs\n",
        ") -> dict:\n",
        "    global data_dict\n",
        "    if is_crypto:\n",
        "      key = crypto_key\n",
        "    else:\n",
        "      key = stocks_key\n",
        "\n",
        "    api = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/{agg_bar_size}/{agg_bar_unit}/{date}/{date}?adjusted=true&sort=asc&limit=1440&apiKey={key}\"\n",
        "    # print(f\"Requesting {api}\")\n",
        "    resp = await session.request('GET', url=api, **kwargs)\n",
        "    # print(resp)\n",
        "    data = await resp.json()\n",
        "    # print(data)\n",
        "    data_dict[date] = data\n",
        "    \n",
        "async def main(dates, **kwargs):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for c in dates:\n",
        "            tasks.append(get(session=session, date=c, **kwargs))\n",
        "        responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "        return responses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7hW564KR5Jl"
      },
      "source": [
        "# Make Cache"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HgR6jT4CeXQ"
      },
      "source": [
        "def convert_responses_to_ohlcv():\n",
        "  print(f\"Response Count: {len(data_dict)}\")\n",
        "  new_dict = []\n",
        "\n",
        "  for index,i in enumerate(data_dict):\n",
        "      if 'results' not in list(data_dict[i].keys()):\n",
        "          pass\n",
        "      else:\n",
        "          new_dict = new_dict + data_dict[i]['results']\n",
        "\n",
        "  return new_dict\n",
        "\n",
        "def make_df(new_dict):\n",
        "  df = pd.DataFrame(new_dict)\n",
        "  df['timestamp'] = pd.to_datetime(df['t'], unit='ms')\n",
        "  df['timestamp'] = df['timestamp'].dt.tz_localize('UTC')\n",
        "  df['timestamp'] = df['timestamp'].dt.tz_convert('US/Eastern')\n",
        "  df['timestamp'] = df['timestamp'].dt.tz_localize(None)\n",
        "  df.sort_values(by='timestamp', ignore_index=True, inplace=True)\n",
        "  # df['timestamp'] = df['timestamp'] + timedelta(minutes=1)\n",
        "\n",
        "  df.sort_values(by='timestamp', ignore_index=True, inplace=True)\n",
        "\n",
        "  df = df[['timestamp','o','h','l','c','v']]\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TVo7fqmSJw3"
      },
      "source": [
        "# Run stocks data tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwRCLWYXvrUK"
      },
      "source": [
        "def get_original_time_difference(dt):\n",
        "  nyse = mcal.get_calendar(\"NYSE\")\n",
        "  tms_df = nyse.schedule(start_date=str(dt), end_date=str(dt))\n",
        "  # print((tms_df['market_close'].iloc[0] - tms_df['market_open'].iloc[0]).total_seconds() / 60)\n",
        "  return (tms_df['market_close'].iloc[0] - tms_df['market_open'].iloc[0]).total_seconds() / 60\n",
        "\n",
        "def check_holidays(row, circuit_breakers):\n",
        "  if row['date'].strftime(\"%Y-%m-%d\") in circuit_breakers:\n",
        "    row['missing'] = 0\n",
        "  elif get_original_time_difference(row['date']) < row['timestamp']:\n",
        "    row['missing'] = 0\n",
        "  else:\n",
        "    row['missing'] = 1\n",
        "\n",
        "  return row\n",
        "\n",
        "def is_stock_data_missing(df):\n",
        "  circuit_breakers = ['2020-03-09', '2020-03-12', '2020-03-16', '2020-03-18'] \n",
        "  df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "  market_hours_df = df.loc[(df['timestamp'].dt.time >= time(9, 30)) & (df['timestamp'].dt.time <= time(16, 0))]\n",
        "  data_count_df = market_hours_df.groupby(by=market_hours_df['timestamp'].dt.date).count()\n",
        "  missing_data_df = data_count_df.loc[data_count_df['timestamp'] < 390]\n",
        "  missing_data_df['date'] = missing_data_df.index\n",
        "\n",
        "  missing_data_df = missing_data_df.apply(check_holidays, args=([circuit_breakers]), axis=1)\n",
        "\n",
        "  if missing_data_df[missing_data_df['missing'] == 1].shape[0] > 0:\n",
        "    print(\"Some Market Hours data is missing!\")\n",
        "    print(missing_data_df[missing_data_df['missing'] == 1])\n",
        "    return missing_data_df\n",
        "\n",
        "  else:\n",
        "    print(\"No market hours data missing! Run write data to bucket section\")\n",
        "    return missing_data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGdbah8SaCVp"
      },
      "source": [
        "# Run crypto data tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilTSU4K_aBTK"
      },
      "source": [
        "def is_crypto_data_missing(df):\n",
        "  data_count_df = df.groupby(df['timestamp'].dt.date).count()\n",
        "  missing_data_df = data_count_df.loc[data_count_df['timestamp'] < 1440]\n",
        "\n",
        "  if missing_data_df.shape[0] > 0:\n",
        "    print(\"Some data is missing!\")\n",
        "    print(missing_data_df)\n",
        "    return True\n",
        "\n",
        "  else:\n",
        "    print(\"No data missing!\")\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVsAj3k0sGAQ"
      },
      "source": [
        "# Write data to bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLHulsiwsLWy"
      },
      "source": [
        "def remove_old_cache_if_already_exists(ticker):\n",
        "  if ':' in ticker:\n",
        "    ticker_to_write = ticker.split(':')[1]\n",
        "  else:\n",
        "    ticker_to_write = ticker\n",
        "\n",
        "  fpaths = glob.glob(f\"/content/BTCacheResampled_{ticker_to_write}*.csv\")\n",
        "\n",
        "  for f in fpaths:\n",
        "    os.remove(f)\n",
        "    print(f\"Removed {f}\")\n",
        "\n",
        "def write_to_bucket(df, ticker):\n",
        "  remove_old_cache_if_already_exists(ticker)\n",
        "\n",
        "  if ':' in ticker:\n",
        "    ticker_to_write = ticker.split(':')[1]\n",
        "  else:\n",
        "    ticker_to_write = ticker\n",
        "\n",
        "  df.to_csv(f\"/content/BTCacheResampled_{ticker_to_write}_{df['timestamp'].iloc[0].strftime('%Y-%m-%d')}_{df['timestamp'].iloc[-1].strftime('%Y-%m-%d')}.csv\", \n",
        "            index=False)\n",
        "\n",
        "  print(f\"File saved at: /content/bucket/Resampled Cache/BTCacheResampled_{ticker_to_write}_{df['timestamp'].iloc[0].strftime('%Y-%m-%d')}_{df['timestamp'].iloc[-1].strftime('%Y-%m-%d')}.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KBq_E-MyJlj"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UitaQGtvyNDc"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  for ticker in tickers:\n",
        "    dates = []\n",
        "    for i in daterange(pd.to_datetime(start_date), pd.to_datetime(end_date)):\n",
        "        dates.append(i.date().strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "    ticker = ticker.upper()\n",
        "    asyncio.run(main(dates))  \n",
        "    print(f\"Data Downloaded\")\n",
        "\n",
        "    n_dict = convert_responses_to_ohlcv()\n",
        "    df = make_df(n_dict)\n",
        "    print(f\"OHLCV Prepared for {ticker}\")\n",
        "\n",
        "    write_to_bucket(df, ticker)\n",
        "\n",
        "    data_dict = {}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}